{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "141e4abe-96d3-42bf-8633-9c4decc028e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storageAccount=\"cookbookstoragegen2\"\n",
    "mountpoint = \"/mnt/Gen2Source\"\n",
    "storageEndPoint =\"abfss://rawdata@{}.dfs.core.windows.net/\".format(storageAccount)\n",
    "print ('Mount Point ='+mountpoint)\n",
    "\n",
    "#ClientId, TenantId and Secret is for the Application(ADLSGen2App) was have created as part of this recipe\n",
    "clientID =\"xxx-xx-xxx\"\n",
    "tenantID =\"xxx-xxx-xxxx\"\n",
    "clientSecret =\"xx-xx-xxx-xxx\"\n",
    "oauth2Endpoint = \"https://login.microsoftonline.com/{}/oauth2/token\".format(tenantID)\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "\"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "\"fs.azure.account.oauth2.client.id\": clientID,\n",
    "\"fs.azure.account.oauth2.client.secret\": clientSecret,\n",
    "\"fs.azure.account.oauth2.client.endpoint\": oauth2Endpoint}\n",
    "\n",
    "dbutils.fs.mount(\n",
    "source = storageEndPoint,\n",
    "mount_point = mountpoint,\n",
    "extra_configs = configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "933532fc-216d-4f65-af8d-946c82a752bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.unmount(\"/mnt/Gen2Source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a701de68-b545-48b3-a917-1d9cee3c6fac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/Gen2Source/Customer/csvFiles\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e814e5-48e8-4885-80a5-5c9cf431674e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "blobStorage = \"cookbookblobstorage1.blob.core.windows.net\"\r\n",
    "blobContainer = \"synapse\"\r\n",
    "blobAccessKey = \"xx-xxx-xxx\"\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49f7e32a-5d19-4c7d-9081-2e43625c10af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tempDir = \"wasbs://\" + blobContainer + \"@\" + blobStorage +\"/tempDirs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d045e00-e3d7-43a3-aad3-4c28b579ca69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "acntInfo = \"fs.azure.account.key.\"+ blobStorage\r\n",
    "#Setting Blob storage acces key for this notebook\r\n",
    "spark.conf.set(\r\n",
    "  acntInfo,\r\n",
    "  blobAccessKey)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "576f943e-42ef-418a-8992-de07f156171b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customerDF = spark.read.format(\"csv\").option(\"header\",True).option(\"inferSchema\", True).load(\"dbfs:/mnt/Gen2Source/Customer/csvFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3812963-8c01-41d1-a86b-2d4e2895dfa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We have changed trustServerCertificate=true from trustServerCertificate=false. In certain cases you might get error \r\n",
    "'''Py4JJavaError: An error occurred while calling o390.save.\r\n",
    ": com.databricks.spark.sqldw.SqlDWSideException: Azure Synapse Analytics failed to execute the JDBC query produced by the connector.\r\n",
    "Underlying SQLException(s):\r\n",
    "  - com.microsoft.sqlserver.jdbc.SQLServerException: The driver could not establish a secure connection to SQL Server by using Secure Sockets Layer (SSL) encryption. Error: \"Failed to validate the server name in a certificate during Secure Sockets Layer (SSL) initialization.\". ClientConnectionId:3a34e73d-7b06-44c6-b23c-a581567f93f5 [ErrorCode = 0] [SQLState = 08S '''\r\n",
    "  \r\n",
    "sqlDwUrl=\"jdbc:sqlserver://synapsedemoworkspace11.sql.azuresynapse.net:1433;database=sqldwpool1;user=sqladminuser@synapsedemoworkspacetest;password=TestStrongPwd;encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\"\r\n",
    "\r\n",
    "db_table = \"dbo.customer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88437362-4a87-448f-9dd6-c41e441536cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code is writing to data into SQL Pool with default save option. In the default save option it check if the table name exists then it errors out else it will create a table and populates the data.\r\n",
    "customerDF.write \\\r\n",
    "  .format(\"com.databricks.spark.sqldw\")\\\r\n",
    "  .option(\"url\", sqlDwUrl)\\\r\n",
    "  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\\\r\n",
    "  .option(\"dbTable\", db_table)\\\r\n",
    "  .option(\"tempDir\", tempDir)\\\r\n",
    "  .save()\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4df2ab2c-db0f-4373-bc95-de1172d445c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code is writing to data into SQL Pool with append save option. In append save option data is appended to existing table.\r\n",
    "customerDF.write \\\r\n",
    "  .format(\"com.databricks.spark.sqldw\")\\\r\n",
    "  .option(\"url\", sqlDwUrl)\\\r\n",
    "  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\\\r\n",
    "  .option(\"dbTable\", db_table)\\\r\n",
    "  .option(\"tempDir\", tempDir)\\\r\n",
    "  .mode(\"append\")\\\r\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d5568ee-40f9-4d2f-9a89-88ee21f494d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This code is writing to data into SQL Pool with over save option. In the default save option it check if the table name exists then it errors out else it will create a table and populates the data.\r\n",
    "customerDF.write \\\r\n",
    "  .format(\"com.databricks.spark.sqldw\")\\\r\n",
    "  .option(\"url\", sqlDwUrl)\\\r\n",
    "  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\\\r\n",
    "  .option(\"dbTable\", db_table)\\\r\n",
    "  .option(\"tempDir\", tempDir)\\\r\n",
    "  .mode(\"overwrite\")\\\r\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53e50903-2ba8-43df-87f2-725dd7a94701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# customerTabledf = spark.read\r\n",
    "#   .format(\"com.databricks.spark.sqldw\")\r\n",
    "#   .option(\"url\", \"jdbc:sqlserver://azureaynapseanalyticssvr.database.windows.net:1433;database=AzureSynapseAnalyticsDW;user=xxx@azureaynapseanalyticssvr;password=xxxx;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\")\r\n",
    "#   .option(\"tempDir\", \"wasbs://rawdata@stacccookbook.blob.core.windows.net/tempDirs\")\r\n",
    "#   .option(\"forwardSparkAzureStorageCredentials\", \"true\")\r\n",
    "#   .option(\"dbTable\", \"CustomerTable\")\r\n",
    "#   .load()\r\n",
    "  \r\n",
    "  # Get some data from an Azure Synapse table.\r\n",
    "customerTabledf = spark.read \\\r\n",
    "  .format(\"com.databricks.spark.sqldw\") \\\r\n",
    "  .option(\"url\", sqlDwUrl) \\\r\n",
    "  .option(\"tempDir\", tempDir) \\\r\n",
    "  .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\r\n",
    "  .option(\"dbTable\", db_table) \\\r\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be90fa71-a644-44f8-a2e8-b0dc762671e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customerTabledf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ca5b2cd-09d1-46cd-91cb-7748b5b02c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query= \" select C_MKTSEGMENT, count(*) as Cnt from [dbo].[customer] group by C_MKTSEGMENT\"\r\n",
    "df_query = spark.read \\\r\n",
    "  .format(\"com.databricks.spark.sqldw\") \\\r\n",
    "  .option(\"url\", sqlDwUrl) \\\r\n",
    "  .option(\"tempDir\", tempDir) \\\r\n",
    "  .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\r\n",
    "  .option(\"query\", query) \\\r\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f25d0c0f-5179-4b40-8152-cf318ca74017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_query.limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_5.Reading and Writing Data from and to Azure Synapse",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
